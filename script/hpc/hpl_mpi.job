#!/bin/bash 

# slurm job script to run xHPL


# expect to be called via a submitter, like
# ./hpl_mpi_submitter.sh
# or just simply 
# sbatch hpl_mpi.job
# sbatch -N 8 hpl_mpi.job
# sbatch -N 2 -w n0282.savio2,n0283.savio2 --partition=savio2_bigmem --account=scs --qos=savio_debug --time=00:05:00 hpl_mpi.job


###  it is not pretty, but don't spend too much time on this, as new node acceptance test cannot run via slurm
###  so just tweak this enough to make test easy enough to run
###  Should just be able to adjust -N 2 to actual number of nodes desired, and the rest of script will adjust accordingly
###  future tweaks CorePerNode when not using 64...


# maybe via Yong's staging.sh
# adopted from run_staging_test_hpl.sh
# and ~/gsHPCS_toolkit/benchmark/hpl/run_hpl_slurm.sh

# cannot have any shell script command before #SBATCH clause, not even vars declaration

#SBATCH                 --job-name=hpl_mpi_test    # CLI arg will overwrite this
#                       CPU time:
#       #SBATCH         --time=605
#                       Wall clock limit in HH:MM:ss
#SBATCH                 --time=11:55:00                    # 
#       #SBATCH         --time=1-02:00:00                  # format for 1 day 2 hours, could also use more than 24 hours
#SBATCH                 --exclusive
#       #SBATCH         --qos=lr_normal
#SBATCH                 --qos=cf_normal
#SBATCH         	--partition=cf1
#	#SBATCH         	-n 128		# did not work for 64 last time.  128 => 3 nodes.  just can't get 64c per node for xhpl when requested like this.
###SBATCH         	-N 2			# adjust as needed   # tested up to -N 16
#SBATCH                 -o  J%j_%N.out
#	#SBATCH                 -o  slr_%j_%N.out
#			http://geco.mines.edu/prototype/How_do_I_manage_jobs/sbatch.html , see --input=

#SBATCH                 --account=scs
#       #SBATCH         --ntasks=2
#       #SBATCH         --mail-type=all
#       #SBATCH         --mail-type=END,FAIL
#       #SBATCH         --mail-type=FAIL
#       #SBATCH         --mail-type=TIME_LIMIT_80 
#SBATCH                 --mail-type=NONE
#SBATCH                 --mail-user=tin@lbl.gov


################################################################################
##### print some info into the slurm output log
################################################################################
###
### info about slurm job
### https://www.glue.umd.edu/hpcc/help/slurmenv.html
### https://slurm.schedmd.com/sbatch.html#lbAH
###
echo "mpirun for xHPL run via slurm"
echo ----slurm-resource-allocated-------------------
echo -n "SLURMD_NODENAME: "
echo    $SLURMD_NODENAME
echo -n "SLURM_JOB_NUM_NODES: "		# Total number of nodes in the job's resource allocation.
echo    $SLURM_JOB_NUM_NODES
echo -n "SLURM_JOB_NODELIST: "
echo    $SLURM_JOB_NODELIST
echo -n "SLURM_TASKS_PER_NODE: "	# eg 64(x2)  , so will be some work before using it for CorePerNode 
echo    $SLURM_TASKS_PER_NODE
echo -n "SLURM_NTASKS_PER_NODE: "   	# Number of tasks requested per core. Only set if the --ntasks-per-core option is specified.
echo    $SLURM_NTASKS_PER_NODE
echo -n "SLURM_MEM_PER_NODE: "		# got blank!
echo    $SLURM_MEM_PER_NODE
echo -n "SLURM_CPUS_ON_NODE: "		# 64 for n0000.cf1 knl
echo    $SLURM_CPUS_ON_NODE
echo -n "SLURM_JOB_PARTITION:"
echo    $SLURM_JOB_PARTITION
echo -n "SLURM_JOB_QOS:"
echo    $SLURM_JOB_QOS
echo ""
echo ""

################################################################################



##JobBaseDir=/global/home/users/tin/gsCF_BK/cf1/staging_test
JobBaseDir=/global/scratch/tin/benchmark_hpl/ 

CorePerNode=64	# knl has 64 cores per node 		# potentially from $SLURM_CPUS_ON_NODE, but not sure if it ever become 63
#NNode=1	# Number of Node to run HPL test on
NNode=${SLURM_JOB_NUM_NODES}
Ncore=$(( $NNode * $CorePerNode ))
HplDatNum=$( printf %04d $NNode )


##FECHA=$(date "+%Y_%m%d_%H%M")
##FECHA=$(date "+%m%d")
FECHA=$(date "+%m%d_%H%M")

##RUNDIR=TMP_OUT
##RUNDIR=STAGING_OUT/HPL_${NNode}_${FECHA}
##RUNDIR=STAGING_OUT/slr_${SLURM_JOB_ID}_hpl${NNode}x${SLURM_CPUS_ON_NODE}_${FECHA}
RUNDIR=./STAGING_OUT/J${SLURM_JOB_ID}_${SLURM_JOB_PARTITION}_${NNode}x${SLURM_CPUS_ON_NODE}_${FECHA}
## won't use till move the submit script too, like slurm output in same place.
##RUNDIR=${JobBaseDir}/STAGING_OUT/J${SLURM_JOB_ID}_${SLURM_JOB_PARTITION}_${NNode}x${SLURM_CPUS_ON_NODE}_${FECHA}	
test -d $RUNDIR || mkdir $RUNDIR
cd $RUNDIR

### make a copy of this script into the run dir since it has param that may want to be refered in future
#echo "job script name/path for making copy?"
#echo $0  # /var/spool/slurmd/job11893476/slurm_script
#cp -pR /var/spool/slurmd/job11893476/slurm_script $RUNDIR   	# can't be found 
cp -p ~/gsCF_BK/cf1/staging_test/hpl_mpi.job .			# path may change... 



BIOSOUT=/tmp/bios.settings.out
BIOSHIGHLIGHT=/tmp/bios.settings.highlight

cp -p $BIOSOUT       .         2> /dev/null
cp -p $BIOSHIGHLIGHT .         2> /dev/null

#cat $BIOSHIGHLIGHT


# undocumented env var for HPL to properly use AVX
# but Josh said they don't do anything  they maybe intel's version of xhpl binary, or some select release?
#export HPL_HOST_ARCH=3 # AVX2
export HPL_HOST_ARCH=9 # AVX512
# KNL has AVX512, may need to dig into these as well...

####
####    specify which version of xhpl binary to use
####

#export PATH=$PATH:~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel64_skylake_avx_optimiz/	# xhpl, no progress report :(
#export PATH=$PATH:~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel64_skylake/		# xhpl

## need new binary...  or use intel's binary??
#export PATH=~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel64_skylake_icc_2018:$PATH

#export PATH=~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel64_phi_7210/:$PATH	# intel 2016 compiler stack
#export PATH=~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel2018_phi_7210:$PATH	# intel 2018 compiler stack
export PATH=~tin/gsHPCS_toolkit/benchmark/hpl/hpl-2.2/bin/intel2018_nehalem:$PATH	# lr4/savio are hashwell, but can't find binary for it...

#export PATH=$PATH:~tin/gsHPCS_toolkit/benchmark/staging_sl7					# mpi_nxnlatbw
#export PATH=$PATH:/global/home/groups/scs/meli/					# osu_*
#export PATH=$PATH:/global/home/groups/scs/yqin					# probably only for stream


####
####    specify what hpl.dat version to use #
####
test -d hpl || mkdir hpl
#cp -p  ~/gsHPCS_toolkit/benchmark/hpl/hpl_cf/64core_192gb_90_nb192/HPL.MPI*dat hpl   # 64 core knl , thus req are in inc of 64cores
#cp -p  ~/gsHPCS_toolkit/benchmark/hpl/hpl_cf/64core_192gb_90_nb336/HPL.MPI*dat hpl    # 64 core knl , fewer .dat than nb192 version for now
cp -p  ~/gsHPCS_toolkit/benchmark/hpl/hpl_cf/24cores_64gb_90_nb192/HPL.MPI*dat hpl   # 24 core lr4/savio2


module purge
#module load gcc openmpi
module load intel openmpi				# lr4/savio2 nehalem hpl
# new intel compiler and lib in personal SMF for now
#module load langs/intel/2018.1.163_eval 
#module load intel/2018.1.163/mkl/2018.1.163_eval
#module load intel/2018.1.163/openmpi/2.0.4-intel_eval
# new module from kai
#module load intel/2018.1.163 openmpi/2.0.2-intel        # not avail in brc yet
module list

numactl -H > numactl-H.out
ompi_info -a > ompi_info-a.out

##cp -p ../../racadmSetBios.sh .



echo "----------- which hpl --------------"
which xhpl
which mpirun

echo "----- mpirun hpl hostname and location  -----"
hostname
pwd
date


echo MPL.dat to copy is hpl/HPL.MPI.${HplDatNum}.dat HPL.dat
cp -p hpl/HPL.MPI.${HplDatNum}.dat HPL.dat
##cp -p hpl/HPL.MPI.0002.dat ./HPL.dat		# tmp forcing a 2 node run even when slurm assigned 3 nodes  ## FIXME

echo "----------- HPL.dat --------------"
cat HPL.dat
echo "----------- mpi run --------------"


### ##############################################################
### actual mpi run
### ##############################################################

echo ""
echo "###"
echo "----------- mpi run 1 - simple, get nodelist --------------"
MpiCmd="mpirun -np ${NNode} -npernode 1 hostname"
##MpiCmd="mpirun -np ${Ncore} -npernode 1 hostname"
##MpiCmd="mpirun -np 8 -npernode 1 hostname"		# actaully don't even works for -np 16, 32 or 128
echo running cmd: ${MpiCmd}
${MpiCmd} | sort -n



echo ""
echo "###"
echo "----------- mpi run 2 - linpack job --------------"
MpiCmd="mpirun -np ${Ncore} -npernode ${CorePerNode} xhpl"
##MpiCmd="mpirun -np 128 -npernode ${CorePerNode} xhpl"			# -n 128 test, which get 3 nodes...   ## FIXME
hostname > HPL.starttime
date 	>> HPL.starttime
echo running cmd: ${MpiCmd}
${MpiCmd} |tee mpi-HPL-2.log


##### method below only work for single node, not 2+ nodes
##### -mca is like loading modules
##### btl sm : sharedmemory Byte Transfer Layer.  which probably why can't run across 2-node MPI.
#echo "----------- mpi run 3 --------------"
#mpirun -np 64 -npernode 64 -bind-to core --report-bindings -mca btl sm,self xhpl 2>&1|tee mpi-HPL.log
#MpiCmd="mpirun -np ${Ncore} -npernode ${CorePerNode} -bind-to core --report-bindings -mca btl sm,self xhpl 2>&1|tee ${RUNDIR}/mpi-HPL.log"
#MpiCmd="mpirun -np ${Ncore} -npernode ${CorePerNode} -bind-to core --report-bindings -mca btl sm,self xhpl"
#echo running cmd: ${MpiCmd}
#${MpiCmd} 2>&1|tee mpi-HPL.log


date > HPL.endtime


echo "end of mpirun for hpl via slurm."
